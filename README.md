# StocastiQ: Stochastic Process Modeling of Datacenter GPU Resource Utilization

## ğŸ“š Project Overview

This project presents an interactive Streamlit dashboard applying **Markov Chains**, **Hidden Markov Models (HMM)**, and **Queueing Theory** to analyze GPU resource utilization and job state transitions in a high-performance computing environment.

We used a curated subset from the **MIT SuperCloud Datacenter Challenge dataset (over 2TB of SLURM job logs)** to explore patterns of resource allocation, job lifecycle, and system load in a real datacenter. Our goal was to model job behaviors, hidden system states, and queueing performance through stochastic process techniques.

---

## ğŸ“‚ Dataset

We accessed the publicly hosted **MIT SuperCloud Datacenter Challenge dataset** via AWS S3 using:

```bash
aws s3 ls s3://datacenter-challenge/202201/ --no-sign-request
```
# Dataset Structure

Inside the dataset we identified:

- **Two main folders:** `cpu/` and `gpu/`  
- **Nested subâ€‘folders:** `0000/` to `0099/` per resource  
- **Paired files for each job:** `*-summary.csv` and `*-timeseries.csv`  

> **Note:** Because the full dataset is ~**2â€¯TB**, we extracted a representative sample of GPUâ€¯*timeseries* andâ€¯*summary* files for analysis.

## Key Features Extracted

| Category | Columns |
|----------|---------|
| **Resource metrics** | `CPUUtilization`, `RSS`, `VMSize`, `IORead`, `IOWrite`, `Threads`, `ElapsedTime` |
| **SLURM job metadata** | `time_submit`, `time_start`, `time_end`, `state`, `cpus_req`, `mem_req`, `partition` |

---

# ğŸ—ï¸ ChallengesÂ &Â Solutions

| Challenge | Our Approach |
|-----------|--------------|
| **Dataset size** | Navigating 2â€¯TB on AWSâ€¯S3 was nonâ€‘trivial. We used the AWSâ€¯CLI with the `--no-sign-request` flag and **selectively downloaded** only the relevant GPU subâ€‘folders for local processing. |
| **Data understanding** | The dataset lacked documentation for state definitions. We combined SLURM state logs with timeseries metrics to **infer discrete states** suitable for modelling. |
| **State representation** | We debated using raw SLURM states vs. derived thresholds. We settled on **discretising `CPUUtilization`** into three states:<br>0â€¯=â€¯*Idle*â€¯(`<â€¯30â€¯%`)<br>1â€¯=â€¯*Normal*â€¯(`30â€¯â€“â€¯70â€¯%`)<br>2â€¯=â€¯*Busy*â€¯(`â‰¥â€¯70â€¯%`) |
| **Model constraints** | Some subsets lacked certain symbols (e.g., only *Idle*). We built **emission matrices** that gracefully handle missing symbols. |
| **Interpreting results** | We **iteratively validated** transition matrices, emission probabilities and steadyâ€‘state distributions against domain expectations. |

---

# ğŸ’» Dashboard Features

| Module | Key Functionality |
|--------|-------------------|
| **Home** | Narrative of dataset journey, challenges & system overview |
| **Markov Chain Analysis** | State transition matrix, steadyâ€‘state distribution, visualisation |
| **Hidden Markov Model** | Emission matrix, steadyâ€‘state, forward & Viterbi algorithms, plots |
| **Queueing Theory** | Arrival/service rates, average wait time, queue simulation |

Each tab provides both **quantitative outputs** (tables, probabilities) and **visual insights** (Matplotlib charts).

---

# ğŸš€ Howâ€¯toâ€¯Run

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/stocastiq.git
cd stocastiq
```
## ğŸ“ Dependencies

- **streamlit**
- **numpy**
- **pandas**
- **hmmlearn**
- **matplotlib**

*Optional*: **AWS CLI** (for dataset fetching)

```bash
pip install streamlit numpy pandas hmmlearn matplotlib
```
# 2. Install dependencies
```
pip install -r requirements.txt
```
# 3. Launch the Streamlit app
```
streamlit run app.py
```
## ğŸ“ Acknowledgments

- **MITâ€¯SuperCloud Datacenter Challenge** team for releasing the dataset  
- **SLURM scheduler documentation** for mapping jobâ€‘state codes  
- Openâ€‘source contributors to **hmmlearn**, **streamlit**, and **matplotlib**
